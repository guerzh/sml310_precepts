{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8T-4oiozODk",
        "colab_type": "code",
        "outputId": "001e83a4-2576-4030-a7f2-c9c232544aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "dat = pd.read_csv(\"https://raw.githubusercontent.com/salesforce/TransmogrifAI/master/helloworld/src/main/resources/IrisDataset/bezdekIris.data\", header = None)\n",
        "dat"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3               4\n",
              "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
              "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
              "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
              "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
              "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
              "..   ...  ...  ...  ...             ...\n",
              "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
              "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
              "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
              "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
              "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MESgqsfzoEM",
        "colab_type": "text"
      },
      "source": [
        "Now, let's convert the data to NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuQk5AaVSZ1j",
        "colab_type": "text"
      },
      "source": [
        "We will be working with the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). The Iris dataset is historically significant -- it was used by Sir Ronald Fisher in demonstrating one of the first classification algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUCpQrP8zvJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_x = dat.loc[:, 0:3].to_numpy()\n",
        "all_y_raw = dat.loc[:, 4].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYp_Omczz1c",
        "colab_type": "text"
      },
      "source": [
        "`all_y_raw` is of type string. We can use `sklearn`'s `LabelEncoder` to convert it to numeric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7A5h4RC0CEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(all_y_raw)\n",
        "all_y = le.transform(all_y_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7upGNgZ0Ru6",
        "colab_type": "text"
      },
      "source": [
        "Now, let's split the data into training, test, and validation sets. First, we'll create an array of indices and permute them. Then we'll take the first 60 indices for the training set, and take 40 indices for the training and 40 indices for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfVrp2nB0MiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = np.random.permutation(range(all_x.shape[0]))\n",
        "train_idx = idx[:60]\n",
        "test_idx = idx[60:100]\n",
        "valid_idx = idx[100:]\n",
        "\n",
        "train_x = all_x[train_idx]\n",
        "train_y = all_y[train_idx]\n",
        "\n",
        "test_x = all_x[test_idx]\n",
        "test_y = all_y[test_idx]\n",
        "\n",
        "valid_x = all_x[valid_idx]\n",
        "valid_y = all_y[valid_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki1iir1X04F6",
        "colab_type": "text"
      },
      "source": [
        "### Problem 0\n",
        "\n",
        "Make sure you understand what `train_idx`, `train_x`, `train_y`, etc. represent.\n",
        "\n",
        "### Problem 1\n",
        "\n",
        "Run Multinomial Logistic Regression (i.e., a no-hidden-layers neural network with the Cross Entroy loss). Report the correct classification rate on the training set and the validation set.\n",
        "\n",
        "### Problem 2\n",
        "\n",
        "Add a hidden layer to the model. Experiment with different sizes of the hidden layer, and show that the performance on the training set becomes better when the hidden layer is larger.\n",
        "\n",
        "### Problem 3\n",
        "\n",
        "Plot the learning curves for the training and validation sets.\n",
        "\n",
        "### Problem 4\n",
        "\n",
        "For the network architecture that worked best, compute the performance on the test set.\n",
        "\n",
        "### Problem 5\n",
        "\n",
        "Check and explain, for a one-hidden-layer neural network that we can compute $$\\sqrt{\\sum_j \\text{weight[j]}^2}$$ using `torch.norm(model[0].weight) + torch.norm(model[2].weight)`\n",
        "\n",
        "Add the term `lam * torch.norm(model[0].weight) + torch.norm(model[2].weight)` to the cost when optimizing. For a large value of `lam`, verify that this makes all the weights 0.\n",
        "\n",
        "(Note that you can also set the `weight_decay` parameter to what you want the $\\lambda$ to be:\n",
        "`optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0.1)`\n",
        "\n",
        "However, this is problematic since that also penalizes large biases, which we do not usually want, so while this is convenient, it should not usually be done.)\n",
        "\n",
        "### Problem 6\n",
        "\n",
        "For a flower with the following characteristics, display the *probabilities* that it is of the species Setosa/Versicolor/Virginica:\n",
        "\n",
        "    SepalLength = 5.8\n",
        "    SepalWidth = 5.8\n",
        "    PetalLength = 5.8\n",
        "    PetalWidth = 5.8\n",
        "\n",
        "\n",
        "Setosa, Versicolor, and Viriginica correspond to 0, 1, and 2 respectively. `SepalLength`, `SepalWidth`, `PetalLength`, and `PetalWidth` correspond to columns 0, 1, 2, and 3 in the data, respectively.\n",
        "\n",
        "### Problem 7\n",
        "\n",
        "For a model with a single layer with three hidden units, take the first hidden unit, and display the weights that connect the inputs to it."
      ]
    }
  ]
}